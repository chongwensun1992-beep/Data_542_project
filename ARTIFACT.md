# ARTIFACT.md â€” MSR 2026 Challenge Track (Anonymous Submission)

## Title  
**Replication Package for MSR 2026 Challenge Track Submission (Anonymous)**

This artifact contains all code, scripts, and notebooks required to fully reproduce the results presented in our MSR 2026 Challenge Track paper. The package includes automated pipelines, engineered features, statistical tests, figures, and reproducible tables for all three research questions (RQ1â€“RQ3).

---

# 1. Artifact Overview

This replication package provides:

- âœ” Fully automated end-to-end pipelines for RQ1â€“RQ3  
- âœ” Clean and analysis-ready CSV tables  
- âœ” All figures used in the paper (generated programmatically)  
- âœ” Statistical analysis (Mannâ€“Whitney U tests)  
- âœ” Interactive Jupyter notebooks  
- âœ” Conda and pip environments for full reproducibility  
- âœ” Deterministic workflows with no manual steps  

The only external dependency is the **AIDev dataset**, which is loaded automatically via HuggingFace using the `hf://` protocol.

---

# 2. System Requirements

- **OS:** Linux, macOS, or Windows  
- **Python:** 3.10  
- **RAM:** â‰¥ 8 GB (recommended: 16 GB)  
- **Internet Access:** Required to load dataset parquet files from HuggingFace  
- **Disk Space:** < 2 GB (only figures and tables are saved locally)

---

# 3. Installation Instructions

## Option A â€” Conda (recommended)

```bash
conda env create -f environment.yml
conda activate msr2026
```
## Option B â€” Pip Installation

If you prefer not to use conda, you can install dependencies directly:

```bash
pip install -r requirements.txt
```
The environment includes core scientific libraries required for all analyses, such as:

- **pandas** â€” data loading and cleaning  
- **numpy** â€” numerical computation  
- **pyarrow** â€” fast parquet access (required for HuggingFace `hf://` loading)  
- **matplotlib** â€” figure generation  
- **seaborn** â€” statistical visualization  
- **scipy** â€” significance testing (e.g., Mannâ€“Whitney U)  
- **tqdm** â€” progress bars for long-running tasks  
- **statsmodels** â€” statistical utilities used during analysis  

These guarantee complete reproducibility across all Research Questions (RQ1â€“RQ3).

---

# 4. Dataset Access

All analyses rely on the **AIDev Dataset (2025 snapshot)**:

ðŸ”— HuggingFace: https://huggingface.co/datasets/hao-li/AIDev

The dataset is accessed programmatically using the HuggingFace filesystem scheme: `hf://hao-li/AIDev/`


No manual download is requiredâ€”scripts load parquet files directly over the network.

âš  **Dataset files are NOT bundled in this artifact**, following MSRâ€™s double-anonymity rules.

---

# 5. Reproducing All Results (Automated Pipeline)

To execute the full workflow for all research questions, run:

```bash
python src/main_run_all.py
```
This generates:

- All figures used across RQ1â€“RQ3  
- All processed and cleaned CSV tables  
- All statistical test outputs (Mannâ€“Whitney U for RQ3)  
- Full reproducibility logs printed in the console  

Outputs are written into the structured directory:
```text
output/
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ RQ1/
â”‚   â”‚   â”œâ”€â”€ rq1_test_inclusion.png
â”‚   â”‚   â”œâ”€â”€ rq1_avg_test_files.png
â”‚   â”‚   â”œâ”€â”€ rq1_three_panel.pdf
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ RQ2/
â”‚   â”‚   â”œâ”€â”€ rq2_comment_distribution.png
â”‚   â”‚   â”œâ”€â”€ rq2_resolution_heatmap.pdf
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ RQ3/
â”‚       â”œâ”€â”€ rq3_features.png
â”‚       â””â”€â”€ ...
```

Every figure and table corresponds directly to the results described in the paper.

---

# 6. Running Individual Research Questions

If you prefer executing one research question at a time, you may run:

### **RQ1 â€” Testing Behavior**
```bash
python src/msr2026/rq1/run_rq1.py
```

### **RQ2 â€” Review Dynamics and Resolution**
```bash
python src/msr2026/rq1/run_rq2.py
```

### **RQ3 â€” Early Acceptance Signals**
```bash
python src/msr2026/rq1/run_rq3.py
```
---

# 7. Jupyter Notebooks (Interactive Replication)

We provide four notebooks:
- 00_setup_environment.ipynb
- 01_rq1_testing_behavior.ipynb
- 02_rq2_review_dynamics.ipynb
- 03_rq3_early_acceptance.ipynb
- 04_full_pipeline.ipynb

### Recommended evaluation order:

1. Run `00_setup_environment` (sanity checks)
2. Explore RQ1â€“RQ3 step-by-step
3. Use `04_full_pipeline` for one-click execution
---

# 8. Reproducibility Claims

| Criterion | Status |
|----------|--------|
| Workflows fully automated | âœ” Yes |
| All outputs generated programmatically | âœ” Yes |
| No manual data editing | âœ” Yes |
| Open-source dependencies only | âœ” Yes |
| Deterministic code paths | âœ” Yes |

---

# 9. License

MIT License (anonymous).

---
# 10. Expected Runtime

Approximate end-to-end running time on a laptop (16GB RAM):

| Component | Runtime      |
|----------|--------------|
| RQ1 pipeline | 1â€“2 minutes  |
| RQ2 pipeline | 1â€“2 minutes  |
| RQ3 pipeline | 1â€“2 minutes  |
| Full pipeline (`main_run_all.py`) | ~3â€“6 minutes |
---
# 11. Repository Structure

```text
MSR-2026/
â”œâ”€â”€ notebooks/                     # Jupyter notebooks (interactive reproduction)
â”‚   â”œâ”€â”€ 00_setup_environment.ipynb
â”‚   â”œâ”€â”€ 01_rq1_testing_behavior.ipynb
â”‚   â”œâ”€â”€ 02_rq2_review_dynamics.ipynb
â”‚   â”œâ”€â”€ 03_rq3_early_acceptance.ipynb
â”‚   â””â”€â”€ 04_full_pipeline.ipynb
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ figures/                   # All generated figures (RQ1â€“RQ3)
â”‚   â””â”€â”€ tables/                    # All generated CSV tables (RQ1â€“RQ3)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ msr2026/                   # Main Python package
â”‚   â”‚   â”œâ”€â”€ rq1/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ run_rq1.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ rq2/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ run_rq2.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ rq3/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ run_rq3.py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ plotting.py
â”‚   â”‚
â”‚   â””â”€â”€ main_run_all.py            # One-click full pipeline
â”‚
â”œâ”€â”€ CITATION.cff                   # Citation metadata
â”œâ”€â”€ LICENSE.txt                    # MIT license (anonymous)
â”œâ”€â”€ README.md                      # Documentation (artifact instructions)
â”œâ”€â”€ environment.yml                # Conda environment definition
â”œâ”€â”€ requirements.txt               # pip dependencies
â”œâ”€â”€ run.sh                         # Shell script for Linux/macOS
â””â”€â”€ run.bat                        # Batch script for Windows

```